{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaraSolace88/CSC386/blob/main/introtoneuralprediction_forwardpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpgYy9ysi5il"
      },
      "source": [
        "# Intro to Neural Prediction and Forward Propagation\n",
        "Presented by Austin O'Brien with figures and examples taken from *Grokking Deep Learning* by Andrew Trask.\n",
        "\n",
        "* We'll begin by observing how the most basic parts of a neural network behave.\n",
        "* Every \"node\" inside of a neural network is fed input, which is then processed, and then the node gives an output.\n",
        "* When using a single node, the input is the data from our dataset itself and the output is the \"prediction\" the neural net makes in the form of a numeric value.\n",
        "\n",
        "* The value itself may be an unbounded numeric value or it may represent a percentage.\n",
        "* Before looking at an entire neural network, let's observe a singlar node.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLzjXt2Gi5io",
        "outputId": "fdc1d999-f7c6-421f-9a75-00a28f557f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "weight = 0.1\n",
        "\n",
        "def neural_network(input, weight):\n",
        "    prediction = input * weight\n",
        "    return prediction\n",
        "\n",
        "num_of_toes = np.array([8.5,9.5,10,9])\n",
        "input = num_of_toes[2]\n",
        "pred = neural_network(input, weight)\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMXkJxxii5io"
      },
      "source": [
        "* We can see the input is a value from the real world. More specifically, information about the real world that we will use to make a prediction, which is the output.\n",
        "* We \"scale\" the input by multiplying it by the weight.\n",
        "  * Multiplying by anything > 1 will increase the output.\n",
        "  * Multiplying by a value between 0 and 1 essentially \"divides\" the input to make the output smaller.\n",
        "  * What does multiplying by 1 do?\n",
        "  * What does multiplying by 0 do?\n",
        "* The output typically isn't correct when we first get started.\n",
        "  * The weights often start as randomized values.\n",
        "  * We will help the neural network tweak the weight to more accurate outputs using training data.\n",
        "* Tweaking the weight up or down to give more accurage predictions is what we mean by \"learning\".\n",
        "* As a thought experiement, what if we want the output to be the negative of the input. As an example, if we have:\n",
        "  * Input: The elevation.\n",
        "  * Output: The probability of spotting a giant squid.\n",
        "  * How should we manipulate the weight?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_4WpWJbi5ip"
      },
      "source": [
        "# Making a prediction with multiple inputs\n",
        "\n",
        "* Above, we were only using a single data point to try and make a prediciton.\n",
        "* Often times, a single predictor isn't enough to make a good prediction, but rather we would like to use multiple predictors at once.\n",
        "\n",
        "* Now, we'll perform a very similar action where we scale each data point by a weight.\n",
        "  * Each data point will have it's own weight associated with it.\n",
        "* We scale each input, and then add all of the products together, which is the output.\n",
        "  * The result of adding the products together is called the *weighted sum of the input*, or **weighted sum** for short. Also referred to as the *dot product*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuO4XDjai5ip",
        "outputId": "878af83f-29ab-4075-9641-4cb4eea6f027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1600000000000001\n"
          ]
        }
      ],
      "source": [
        "# Code Together\n",
        "import numpy as np\n",
        "\n",
        "weights = np.array([0.1, 0.2, 0])\n",
        "\n",
        "def w_sum(inputs, weights):\n",
        "  assert(len(inputs) == len(weights))\n",
        "  output = 0\n",
        "  for i in range(len(inputs)):\n",
        "    output += inputs[i] * weights[i]\n",
        "\n",
        "  return output\n",
        "\n",
        "def neural_network(inputs, weights):\n",
        "  prediction = w_sum(inputs, weights)\n",
        "  return prediction\n",
        "\n",
        "toes = np.array([8.5, 9.5, 10, 9])\n",
        "wlrec = np.array([0.65, .8, .8, .9])\n",
        "nFans = np.array([1.2, 1.3, 0.5, 1.0]) # scaled from hundreds of thousands\n",
        "\n",
        "team_num = 2\n",
        "inputs = np.array([toes[team_num], wlrec[team_num], nFans[team_num]])\n",
        "pred = neural_network(inputs, weights)\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUxbWOkVi5ip"
      },
      "source": [
        "* We're entering the territory of linear algebra, specifically vectors. Don't let yourself be intimidated, a vector is just a list of numbers.\n",
        "  * Vectors are great for performing mathematics on groups of numbers.\n",
        "  * Most vector mathematics require the vectors to be of the same length, but not always.\n",
        "* The weighted sum of two vectors is where we mutiply the elements in the same position (position 0 with 0, position 1 with 1, and so on) and then add these products together.\n",
        "* Anytime you perform math operations on two vectors of equal length and pair up the values by position, the operation is called an *elementwise* operation.\n",
        "  * Elementwise addition sums two vectors.\n",
        "  * Elementwise multiplication multiplies two vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lGukjWfi5ip",
        "outputId": "717b66c8-823b-4a31-bd3a-568e1b39111d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elementwise_multiplication\n",
            "[ 4 10 18]\n",
            "elementwise_addition\n",
            "[ 4 10 18]\n",
            "vector_sum\n",
            "6\n",
            "15\n",
            "vector_average\n",
            "2.0\n",
            "5.0\n",
            "dot_product\n",
            "32\n"
          ]
        }
      ],
      "source": [
        "# Challenge Exercises - Test Your Might!!\n",
        "\n",
        "vec_a = np.array([1, 2, 3])\n",
        "vec_b = np.array([4, 5, 6])\n",
        "\n",
        "def elementwise_multiplication(vec_a, vec_b):\n",
        "  assert(len(vec_a) == len(vec_b))\n",
        "  output = [0, 0, 0]\n",
        "  for i in range(len(vec_a)):\n",
        "    output[i] = vec_a[i] * vec_b[i]\n",
        "  return np.array(output);\n",
        "\n",
        "def elementwise_addition(vec_a, vec_b):\n",
        "  assert(len(vec_a) == len(vec_b))\n",
        "  output = [0, 0, 0]\n",
        "  for i in range(len(vec_a)):\n",
        "    output[i] = vec_a[i] * vec_b[i]\n",
        "  return np.array(output);\n",
        "\n",
        "def vector_sum(vec_a):\n",
        "  output = 0\n",
        "  for i in range(len(vec_a)):\n",
        "    output += vec_a[i]\n",
        "  return output\n",
        "\n",
        "def vector_average(vec_a):\n",
        "  output = 0\n",
        "  for i in range(len(vec_a)):\n",
        "    output += vec_a[i]\n",
        "  return output / len(vec_a)\n",
        "\n",
        "def dot_product(vec_a, vec_b):\n",
        "  output = 0\n",
        "  return vector_sum(elementwise_multiplication(vec_a, vec_b))\n",
        "\n",
        "print(\"elementwise_multiplication\")\n",
        "print(elementwise_multiplication(vec_a, vec_b))\n",
        "print(\"elementwise_addition\")\n",
        "print(elementwise_addition(vec_a, vec_b))\n",
        "print(\"vector_sum\")\n",
        "print(vector_sum(vec_a))\n",
        "print(vector_sum(vec_b))\n",
        "print(\"vector_average\")\n",
        "print(vector_average(vec_a))\n",
        "print(vector_average(vec_b))\n",
        "print(\"dot_product\")\n",
        "print(dot_product(vec_a, vec_b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Miwnq_i5iq"
      },
      "source": [
        "Use the following vectors as inputs and test your functions.\n",
        "\n",
        "Make sure you get the same results for the weighted sum examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqjwIWjGi5iq"
      },
      "source": [
        "* So, what exactly does a weighted sum tell us? Let's look at some of the interesting properties of the weighted sum of two vectors.\n",
        "\n",
        "* First, let's observe what particular values in the vectors do to the final result.\n",
        "  * Values of zero completely nullify any impact of the paired value on the result.\n",
        "  * Values greater than zero but less than one (decimals) lessen the impact of the paired value on the result.\n",
        "  * Values of 1 do no transfer on the paired value.\n",
        "  * Values greater than 1 increase the impact of the paired value.\n",
        "  * Negative values reverse the impact.\n",
        "  \n",
        "* The author likes to thing of the weighted sum as a *notion of similarity* between the two vectors. Let's review the examples above:\n",
        "  * The largest weighted sum is w_sum(c,c), while the lowest are w_sum(a,b) and w_sum(c,e). Given the notion of similarity, why does this make sense?\n",
        "  \n",
        "* The author goes on to explain the similarities between a dot product and a logical AND operator.\n",
        "  * For 1s and 0s, this is fairly intuitive. If the weight is a zero, then the similarity score is unaffected, otherwise a 1 increasese the score.\n",
        "  * But for decimal values, we can think of it as a 'partial ANDing', where we get a reduced similarity score.\n",
        "  * For values larger than one, we can think of it as 'boosted ANDing', where we get an increased similarity score.\n",
        "  \n",
        "* Negative values can be thought of as a logical NOT operator.\n",
        "  * A positive weight paired with a negative weight cuases the similarity score to go down.\n",
        "  * A double negative will add to the score.\n",
        "  \n",
        "* Continuing with the logical equivalences, the author mentions that we can insert ORs between weights to create a \"crude language\" of sorts. For example:\n",
        "  * weights = [ 1, 0, 1] => if input[0] OR input[2]\n",
        "  * weights = [ 1, 0, -1] => if input[0] OR NOT input[2]\n",
        "  * weights = [ 0.5, 0, 1] => if BIG input[0] or input[2].  Note how a weight of 0.5 means the corresponding input needs to be larger to compensate.\n",
        "  \n",
        "* While a crude langauge, this helps us give some context to what the weights mean, and how they affect the output given the input.\n",
        "  * This is to say, a high output score is given when the weights are similar to their corresponding input values.\n",
        "  \n",
        "* Given our example with num_toes, wlrec, and nfans; what parameter has the least affect on the score? How about the most affect?\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "Y0Uy122ii5iq"
      },
      "outputs": [],
      "source": [
        "# For the fun of it, let's simplify the neural_network function so we're using the built in numpy dot product function instead of our w_sum function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozRRkjwKi5iq"
      },
      "source": [
        "# Making a prediction with multiple outputs\n",
        "* It's also possible to get multiple outputs from a neural network, even from a single input.\n",
        "\n",
        "* Here we can see the wlrec is used to determine multiple outputs:\n",
        "  * What percentage of the team is hurt?\n",
        "  * Will the team win the next game?\n",
        "  * Are the players happy or sad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "Jbr1YrR-i5iq"
      },
      "outputs": [],
      "source": [
        "# Code multiple outputs together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiT0iHgui5ir"
      },
      "source": [
        "# Predicting with multiple inputs and outputs\n",
        "\n",
        "* We can combine the two methods to get multiple outputs with multiple inputs as well.\n",
        "* Each input with have a weight associated with each output.\n",
        "* The methodology is essentially the same as before, where now each output will be the dot product of it's corresponding inputs and weights.\n",
        "\n",
        "* We're going to combine multiple vectors together to make matrices. Again, don't be intimidated, a matrix is just a several vectors placed together.\n",
        "* And now we can simply perform three different dot products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "dcZE6Ab1i5ir"
      },
      "outputs": [],
      "source": [
        "# Let's write the code using matrices to get multple outputs with multiple inputs\n",
        "import numpy as np\n",
        "\n",
        "                   #toes %win #fans\n",
        "weights = np.array([ [0.1,0.1,-0.3],   # hurt?\n",
        "                     [0.1,0.2, 0.0],   # win?\n",
        "                     [0.0,1.3, 0.1] ]) # sad?\n",
        "\n",
        "toes = np.array([8.5,9.5,10,9])\n",
        "wlrec = np.array([0.65,0.8,0.8,0.9])\n",
        "nfans = np.array([1.2,1.3,0.5,1.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAFG_c2wi5ir"
      },
      "source": [
        "* As you saw, we used the matrix to transport our vectors to our neural network function, but once there, we only used one vector at a time from the matrix to get the dot products.\n",
        "\n",
        "# Stacked Neural Networks\n",
        "\n",
        "* Now we'll see that we can take the output from one network and use it as input for another network. This creates a 'stacked', or **deep neural network**.\n",
        "* We refer to a new set of nodes as a layer.\n",
        "  * We'll need to do a set of vector-matrix multiplication for each layer after the input.\n",
        "* I realize it's not apparent why we might want to stack layers like this to make predictions, but we'll study it later on in the book.\n",
        "  * For now, think of more layers as allowing us to model more complex patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "J_6cd4kzi5ir"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "                   #toes %win #fans\n",
        "ih_wgt = np.array([ [0.1, 0.2, -0.1],    # hid[0]\n",
        "                    [-0.1,0.1, 0.9],     # hid[1]\n",
        "                    [0.1, 0.4, 0.1] ])   # hid[2]\n",
        "\n",
        "                # hid[0] hid[1] hid[2]\n",
        "hp_wgt = np.array([ [0.3, 1.1, -0.3],    # hurt?\n",
        "                    [0.1, 0.2, 0.0],     # win?\n",
        "                    [0.0, 1.3, 0.1] ])   # sad?\n",
        "\n",
        "\n",
        "toes = np.array([8.5,9.5,10,9])\n",
        "wlrec = np.array([0.65,0.8,0.8,0.9])\n",
        "nfans = np.array([1.2,1.3,0.5,1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "jy9NU1lOi5ir"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.zeros((2,4))         #1\n",
        "b = np.zeros((4,3))         #2\n",
        "c = a.dot(b)\n",
        "print(c.shape)              #3\n",
        "\n",
        "e = np.zeros((2,1))         #4\n",
        "f = np.zeros((1,3))         #5\n",
        "g = e.dot(f)\n",
        "print(g.shape)              #6\n",
        "\n",
        "h = np.zeros((5,4)).T       #7 8\n",
        "i = np.zeros((5,6))         #9\n",
        "j = h.dot(i)\n",
        "print(j.shape)              #10\n",
        "\n",
        "h = np.zeros((5,4))         #11\n",
        "i = np.zeros((5,6))         #12\n",
        "j = h.dot(i)\n",
        "print(j.shape)              #13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8eNurRki5ir"
      },
      "source": [
        "* What we've done so far is called *forward propagation*, where a neural network takes input take and makes a prediction.\n",
        "  * We're propagating activations forward through the network.\n",
        "  * An **activation** are the numbers that aren't weights going through the network; the outputs from each node.\n",
        "* Our goal now is to set the weights to make accurate predictions.\n",
        "* *Weight Learning* is how we autotune the weights and is also a series of simple techniques combined many times across the architecture.\n",
        "\n",
        "~ Fin"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}